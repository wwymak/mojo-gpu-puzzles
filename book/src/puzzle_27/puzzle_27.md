# Puzzle 27: Block-Level Programming

## Overview

Welcome to **Puzzle 27: Block-Level Programming**! This puzzle introduces you to the fundamental building blocks of GPU parallel programming - **block-level communication primitives** that enable sophisticated parallel algorithms across entire thread blocks. You'll explore three essential communication patterns that replace complex manual synchronization with elegant, hardware-optimized operations.

**What you'll achieve:** Transform from complex shared memory + barriers + tree reduction patterns (Puzzle 12) to elegant single-function-call algorithms that leverage hardware-optimized block-wide communication primitives across multiple warps.

**Key insight:** _GPU thread blocks execute with sophisticated hardware coordination - Mojo's block operations harness cross-warp communication and dedicated hardware units to provide complete parallel programming building blocks: reduction (allâ†’one), scan (allâ†’each), and broadcast (oneâ†’all)._

## What you'll learn

### **Block-level communication model**
Understand the three fundamental communication patterns within GPU thread blocks:

```
GPU Thread Block (128 threads across 4 or 2 warps, hardware coordination)
All-to-One (Reduction):     All threads â†’ Single result at thread 0
All-to-Each (Scan):         All threads â†’ Each gets cumulative position
One-to-All (Broadcast):     Thread 0 â†’ All threads get same value

Cross-warp coordination:
â”œâ”€â”€ Warp 0 (threads 0-31)   â”€â”€block.sum()â”€â”€â”
â”œâ”€â”€ Warp 1 (threads 32-63)  â”€â”€block.sum()â”€â”€â”¼â†’ Thread 0 result
â”œâ”€â”€ Warp 2 (threads 64-95)  â”€â”€block.sum()â”€â”€â”¤
â””â”€â”€ Warp 3 (threads 96-127) â”€â”€block.sum()â”€â”€â”˜
```

**Hardware reality:**
- **Cross-warp synchronization**: Automatic coordination across multiple warps within a block
- **Dedicated hardware units**: Specialized scan units and butterfly reduction networks
- **Zero explicit barriers**: Hardware manages all synchronization internally
- **Logarithmic complexity**: \\(O(\\log n)\\) algorithms with single-instruction simplicity

### **Block operations in Mojo**
Learn the complete parallel programming toolkit from `gpu.block`:

1. **[`block.sum(value)`](https://docs.modular.com/mojo/stdlib/gpu/block/sum)**: All-to-one reduction for totals, averages, maximum/minimum values
2. **[`block.prefix_sum(value)`](https://docs.modular.com/mojo/stdlib/gpu/block/prefix_sum)**: All-to-each scan for parallel filtering and extraction
3. **[`block.broadcast(value)`](https://docs.modular.com/mojo/stdlib/gpu/block/broadcast)**: One-to-all distribution for parameter sharing and coordination

> **Note:** These primitives enable sophisticated parallel algorithms like statistical computations, histogram binning, and normalization workflows that would otherwise require dozens of lines of complex shared memory coordination code.

### **Performance transformation example**
```mojo
# Complex block-wide reduction (traditional approach - from Puzzle 12):
shared_memory[local_i] = my_value
barrier()
for stride in range(64, 0, -1):
    if local_i < stride:
        shared_memory[local_i] += shared_memory[local_i + stride]
    barrier()
if local_i == 0:
    output[block_idx.x] = shared_memory[0]

# Block operations eliminate all this complexity:
my_partial = compute_local_contribution()
total = block.sum[block_size=128, broadcast=False](my_partial)  # Single call!
if local_i == 0:
    output[block_idx.x] = total[0]
```

### **When block operations excel**
Learn the performance characteristics:

| Algorithm Pattern | Traditional | Block Operations |
|-------------------|-------------|------------------|
| Block-wide reductions | Shared memory + barriers | Single `block.sum` call |
| Parallel filtering | Complex indexing | `block.prefix_sum` coordination |
| Parameter sharing | Manual synchronization | Single `block.broadcast` call |
| Cross-warp algorithms | Explicit barrier management | Hardware-managed coordination |

## The evolution of GPU programming patterns

### **Where we started: Manual coordination (Puzzle 12)**
Complex but educational - explicit shared memory, barriers, and tree reduction:
```mojo
# Manual approach: 15+ lines of complex synchronization
shared_memory[local_i] = my_value
barrier()
# Tree reduction with stride-based indexing...
for stride in range(64, 0, -1):
    if local_i < stride:
        shared_memory[local_i] += shared_memory[local_i + stride]
    barrier()
```

### **The intermediate step: Warp programming (Puzzle 24)**
Hardware-accelerated but limited scope - `warp.sum()` within 32-thread warps:
```mojo
# Warp approach: 1 line but single warp only
total = warp.sum[warp_size=WARP_SIZE](val=partial_product)
```

### **The destination: Block programming (This puzzle)**
Complete toolkit - hardware-optimized primitives across entire blocks:
```mojo
# Block approach: 1 line across multiple warps (128+ threads)
total = block.sum[block_size=128, broadcast=False](val=partial_product)
```

## The three fundamental communication patterns

Block-level programming provides three essential primitives that cover all parallel communication needs:

### **1. All-to-One: Reduction (`block.sum()`)**
- **Pattern**: All threads contribute â†’ One thread receives result
- **Use case**: Computing totals, averages, finding maximum/minimum values
- **Example**: Dot product, statistical aggregation
- **Hardware**: Cross-warp butterfly reduction with automatic barriers

### **2. All-to-Each: Scan (`block.prefix_sum()`)**
- **Pattern**: All threads contribute â†’ Each thread receives cumulative position
- **Use case**: Parallel filtering, stream compaction, histogram binning
- **Example**: Computing write positions for parallel data extraction
- **Hardware**: Parallel scan with cross-warp coordination

### **3. One-to-All: Broadcast (`block.broadcast()`)**
- **Pattern**: One thread provides â†’ All threads receive same value
- **Use case**: Parameter sharing, configuration distribution
- **Example**: Sharing computed mean for normalization algorithms
- **Hardware**: Optimized distribution across multiple warps


## Learning progression

Complete this puzzle in three parts, building from simple to sophisticated:

### **Part 1: [Block.sum() Essentials](./block_sum.md)**
**Transform complex reduction to simple function call**

Learn the foundational block reduction pattern by implementing dot product with `block.sum()`. This part shows how block operations replace 15+ lines of manual barriers with a single optimized call.

**Key concepts:**
- Block-wide synchronization across multiple warps
- Hardware-optimized reduction patterns
- Thread 0 result management
- Performance comparison with traditional approaches

**Expected outcome:** Understand how `block.sum()` provides warp.sum() simplicity at block scale.

---

### **Part 2: [Block.prefix_sum() Parallel Histogram](./block_prefix_sum.md)**
**Advanced parallel filtering and extraction**

Build sophisticated parallel algorithms using `block.prefix_sum()` for histogram binning. This part demonstrates how prefix sum enables complex data reorganization that would be difficult with simple reductions.

**Key concepts:**
- Parallel filtering with binary predicates
- Coordinated write position computation
- Advanced partitioning algorithms
- Cross-thread data extraction patterns

**Expected outcome:** Understand how `block.prefix_sum()` enables sophisticated parallel algorithms beyond simple aggregation.

---

### **Part 3: [Block.broadcast() Vector Normalization](./block_broadcast.md)**
**Complete workflow combining all patterns**

Implement vector mean normalization using the complete block operations toolkit. This part shows how all three primitives work together to solve real computational problems with mathematical correctness.

**Key concepts:**
- One-to-all communication patterns
- Coordinated multi-phase algorithms
- Complete block operations workflow
- Real-world algorithm implementation

**Expected outcome:** Understand how to compose block operations for sophisticated parallel algorithms.

## Why block operations matter

### **Code simplicity transformation:**
```
Traditional approach:  20+ lines of barriers, shared memory, complex indexing
Block operations:      3-5 lines of composable, hardware-optimized primitives
```

### **Performance advantages:**
- **Hardware optimization**: Leverages GPU architecture-specific optimizations
- **Automatic synchronization**: Eliminates manual barrier placement errors
- **Composability**: Operations work together seamlessly
- **Portability**: Same code works across different GPU architectures

### **Educational value:**
- **Conceptual clarity**: Each operation has a clear communication purpose
- **Progressive complexity**: Build from simple reductions to complex algorithms
- **Real applications**: Patterns used extensively in scientific computing, graphics, AI

## Prerequisites

Before starting this puzzle, you should have completed:
- **[Puzzle 12](../puzzle_12/puzzle_12.md)**: Understanding of manual GPU synchronization
- **[Puzzle 24](../puzzle_24/puzzle_24.md)**: Experience with warp-level programming

## Expected learning outcomes

After completing all three parts, you'll understand:

1. **When to use each block operation** for different parallel communication needs
2. **How to compose operations** to build sophisticated algorithms
3. **Performance trade-offs** between manual and automated approaches
4. **Real-world applications** of block-level programming patterns
5. **Architecture-independent programming** using hardware-optimized primitives

## Getting started

**Recommended approach:** Complete the three parts in sequence, as each builds on concepts from the previous parts. The progression from simple reduction â†’ advanced partitioning â†’ complete workflow provides the optimal learning path for understanding block-level GPU programming.

ðŸ’¡ **Key insight**: Block operations represent the sweet spot between programmer productivity and hardware performance - they provide the simplicity of high-level operations with the efficiency of carefully optimized low-level implementations. This puzzle teaches you to think at the right abstraction level for modern GPU programming.
