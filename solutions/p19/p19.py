import time
import torch
from pathlib import Path
from max.torch import CustomOpLibrary

# Cache the CustomOpLibrary to avoid recompilation
_mojo_ops_cache = None
_compiled_ops_cache = {}

def get_mojo_ops():
    """Get cached CustomOpLibrary instance to avoid recompilation."""
    global _mojo_ops_cache
    if _mojo_ops_cache is None:
        mojo_kernels = Path(__file__).parent / "op"
        _mojo_ops_cache = CustomOpLibrary(mojo_kernels)
        print("Loaded Mojo operations library")
    return _mojo_ops_cache

def get_compiled_embedding_op(batch_size, seq_len, vocab_size, embed_dim, op_name="embedding"):
    """Get cached compiled embedding operation."""
    key = (op_name, batch_size, seq_len, vocab_size, embed_dim)
    if key not in _compiled_ops_cache:
        ops = get_mojo_ops()
        if op_name == "embedding":
            embedding_op = ops.embedding[{
                "batch_size": batch_size,
                "seq_len": seq_len,
                "vocab_size": vocab_size,
                "embed_dim": embed_dim
            }]
        elif op_name == "embedding_2d":
            embedding_op = ops.embedding_2d[{
                "batch_size": batch_size,
                "seq_len": seq_len,
                "vocab_size": vocab_size,
                "embed_dim": embed_dim
            }]
        else:
            raise ValueError(f"Unknown op_name: {op_name}")

        _compiled_ops_cache[key] = torch.compile(embedding_op)
        print(f"Compiled {op_name} op for shape {(batch_size, seq_len, vocab_size, embed_dim)}")
    return _compiled_ops_cache[key]

def embedding_mojo_1d(indices: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
    """1D coalesced embedding kernel"""
    batch_size, seq_len = indices.shape
    vocab_size, embed_dim = weights.shape

    output = torch.empty(
        (batch_size, seq_len, embed_dim),
        dtype=weights.dtype,
        device=weights.device
    )

    if indices.dtype != torch.int32:
        indices = indices.to(torch.int32)

    embedding_op = get_compiled_embedding_op(batch_size, seq_len, vocab_size, embed_dim, "embedding")
    embedding_op(output, indices, weights)
    return output

def embedding_mojo_2d(indices: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
    """2D non-coalesced embedding kernel"""
    batch_size, seq_len = indices.shape
    vocab_size, embed_dim = weights.shape

    output = torch.empty(
        (batch_size, seq_len, embed_dim),
        dtype=weights.dtype,
        device=weights.device
    )

    if indices.dtype != torch.int32:
        indices = indices.to(torch.int32)

    embedding_op = get_compiled_embedding_op(batch_size, seq_len, vocab_size, embed_dim, "embedding_2d")
    embedding_op(output, indices, weights)
    return output


if __name__ == "__main__":
    print("Puzzle 19: Mojo Embedding Kernel Comparison")
    print("=" * 70)
    print()

    batch_size, seq_len, vocab_size, embed_dim = 8, 512, 10000, 512
    print(f"Configuration: B={batch_size}, L={seq_len}, V={vocab_size}, E={embed_dim}")
    print("-" * 60)

    torch.manual_seed(42)
    indices = torch.randint(0, vocab_size, (batch_size, seq_len),
                           device="cuda", dtype=torch.int32)

    embed_layer = torch.nn.Embedding(vocab_size, embed_dim,
                                    device="cuda", dtype=torch.float32)
    weights = embed_layer.weight.data

    print("Testing Correctness...")

    # PyTorch reference for correctness
    ref_output = embed_layer(indices)

    try:
        # Test 1D coalesced kernel
        mojo_1d_output = embedding_mojo_1d(indices, weights)
        max_diff_1d = (ref_output - mojo_1d_output).abs().max().item()
        print(f"   1D Coalesced - Max difference: {max_diff_1d:.2e}")

        # Test 2D non-coalesced kernel
        mojo_2d_output = embedding_mojo_2d(indices, weights)
        max_diff_2d = (ref_output - mojo_2d_output).abs().max().item()
        print(f"   2D Non-coalesced - Max difference: {max_diff_2d:.2e}")

        if max_diff_1d < 1e-5 and max_diff_2d < 1e-5:
            print("   âœ… Both implementations CORRECT")
        else:
            print("   âŒ One or both implementations INCORRECT")
            exit(1)

    except Exception as e:
        print(f"   âŒ Implementation failed: {e}")
        exit(1)

    print()
    print("Benchmarking Mojo Kernels...")

    # Warmup
    for _ in range(5):
        _ = embedding_mojo_1d(indices, weights)
        _ = embedding_mojo_2d(indices, weights)

    torch.cuda.synchronize()

    # Benchmark function
    def benchmark_fn(fn, *args, num_trials=10):
        torch.cuda.synchronize()
        start_time = time.perf_counter()

        for _ in range(num_trials):
            _ = fn(*args)

        torch.cuda.synchronize()
        end_time = time.perf_counter()

        avg_time_ms = (end_time - start_time) * 1000 / num_trials
        return avg_time_ms

    # Benchmark both kernels
    time_1d = benchmark_fn(embedding_mojo_1d, indices, weights)
    time_2d = benchmark_fn(embedding_mojo_2d, indices, weights)

    print()
    print("ðŸ“Š Performance Results:")
    print(f"   1D Coalesced:     {time_1d:.3f} ms")
    print(f"   2D Non-coalesced: {time_2d:.3f} ms")

    if time_1d < time_2d:
        speedup = time_2d / time_1d
        print(f"   ðŸš€ 1D is {speedup:.2f}x faster than 2D")
    else:
        speedup = time_1d / time_2d
        print(f"   âš ï¸ 2D is {speedup:.2f}x faster than 1D")

    print()
    print("ðŸ“š Key Learning Points:")
    print("â€¢ Compare different GPU kernel implementations")
    print("â€¢ 1D vs 2D grid patterns have different memory access")
    print("â€¢ Coalesced memory access should be faster")
    print("â€¢ Grid configuration affects GPU utilization")
